# ğŸ–¼ï¸ Image Captioning Generator

An end-to-end deep learning project that generates **descriptive, human-like captions for images** using an **encoderâ€“decoder architecture with attention mechanisms**.

---

## ğŸ“Œ Project Overview

This project implements an **Image Captioning System** capable of understanding visual content and generating meaningful textual descriptions. It combines computer vision and natural language processing techniques to bridge the gap between images and language.

### Core Components
- **Encoder (CNN):** Extracts high-level visual features from images
- **Decoder (RNN / LSTM):** Generates captions word-by-word
- **Attention Mechanism:** Focuses on relevant regions of the image while generating each word

The project supports **quantitative evaluation** using BLEU scores and **qualitative analysis** through attention heatmap visualizations.

---

## âœ¨ Features

- End-to-end trainable image captioning model
- Caption generation for unseen images
- Attention visualization for interpretability
- Evaluation using BLEU-1 to BLEU-4 metrics
- Modular and extensible codebase for experimentation

---

## ğŸ“Š Results & Visualizations

### ğŸ“ˆ Training Convergence
The following plot illustrated the reduction in Cross-Entropy Loss over the training epochs. The convergence of the validation loss indicates the model's ability to generalize to unseen images.

<p align="center">
  <img src="images/new_loss_curve.png" width="600" alt="Training Loss Curve">
</p>

### ğŸ–¼ï¸ Model Predictions
Below are five qualitative results showing the captions generated by the model on the test dataset.

| Sample 1 | Sample 2 | Sample 3 |
| :---: | :---: | :---: |
| <img src="images/output_image1.png" width="350"> | <img src="images/output_image2.png" width="350"> | <img src="images/output_image3.png" width="350"> |
| **"a and propeller plane on tarmac runway mountains"** | **"a woman holds a dish with food in it"** | **"a and motorcycle on sidewalk parked"** |

<br>

| Sample 4 | Sample 5 |
| :---: | :---: |
| <img src="images/output_image4.png" width="350"> | <img src="images/output_image5.png" width="350"> |
| **"four urinals in a public restroom with window"** | **"a car street a and bus"** |

---

### ğŸ” Attention Heatmap Visualization
The attention mechanism allows the model to focus on specific image regions. In the heatmaps below, the brighter areas indicate where the model placed its "focus" while generating the corresponding word.

<p align="center">
  <img src="images/heatmap_image-2.png" width="900" alt="Attention Heatmap 1">
  <img src="images/heatmap_other.png" width="900" alt="Attention Heatmap 2">
  <br>
  <i>Visualizing spatial focus during word generation</i>
</p>

---

## ğŸ§± Project Structure

```text
image-captioning/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/            # Raw image dataset
â”‚   â”œâ”€â”€ captions/          # Caption files (JSON / CSV / TXT)
â”‚   â””â”€â”€ processed/         # Preprocessed data & tokenized captions
â”‚
â”œâ”€â”€ notebooks/             # Jupyter notebooks for exploration & analysis
â”‚
â”œâ”€â”€ src/                   # Core source code
â”‚   â”œâ”€â”€ dataset.py         # Dataset loading & preprocessing
â”‚   â”œâ”€â”€ model.py           # Encoder, Decoder & Attention models
â”‚   â”œâ”€â”€ train.py           # Training loop
â”‚   â”œâ”€â”€ evaluate.py        # Caption generation & evaluation
â”‚   â”œâ”€â”€ utils.py           # Helper utilities
â”‚   â””â”€â”€ visualize.py       # Attention visualization
â”‚
â”œâ”€â”€ checkpoints/           # Saved model weights
â””â”€â”€ outputs/               # Generated captions, plots & reports
```
---
## ğŸ› ï¸ Tech Stack

| Category | Technologies |
|--------|--------------|
| ğŸ¨ **Vision Encoder** | ğŸ§  CNN (ResNet / Custom CNN) |
| ğŸ“ **Language Decoder** | ğŸ” RNN / ğŸ§  LSTM |
| ğŸ¯ **Attention Mechanism** | ğŸ¯ Soft Attention |
| ğŸ§ª **Frameworks** | ğŸ Python &nbsp;â€¢&nbsp; ğŸ”¥ PyTorch / ğŸ§  TensorFlow |
| ğŸ“Š **Evaluation Metrics** | ğŸ“ BLEU (1â€“4) |
| ğŸ§° **Tools & Utilities** | ğŸ““ Jupyter &nbsp;â€¢&nbsp; ğŸ§‘â€ğŸ’» Git &nbsp;â€¢&nbsp; ğŸŒ GitHub &nbsp;â€¢&nbsp; ğŸ§ª Virtual Environment (venv) |

---

## ğŸ› ï¸ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/your-username/image-captioning.git
cd image-captioning
```
### 2ï¸âƒ£ Create and activate a virtual environment
```
python -m venv venv
```
- Linux / macOS
```
source venv/bin/activate
```
- Windows
```
venv\Scripts\activate
```

--- 

### 3ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```

---

## ğŸš€ Usage

### ğŸ”¹ Train the Model
```bash
python src/train.py --data_path data/ --epochs 20 --batch_size 32
```
### ğŸ”¹ Generate Caption for a Single Image
```
python src/evaluate.py \
  --image_path data/images/sample.jpg \
  --checkpoint checkpoints/best_model.pth
```
### ğŸ”¹ Visualize Attention Maps 
```
python src/visualize.py \
  --image_path data/images/sample.jpg \
  --caption "a man riding a snowboard" \
  --alphas alphas.npy
```

---
## ğŸ“Š Evaluation Metrics

The model is evaluated using standard image captioning metrics to measure caption quality and accuracy.

- **BLEU-1 to BLEU-4** â€“ Measures n-gram overlap between generated and reference captions.

### Optional Metrics
- **METEOR**
- **ROUGE**
- **CIDEr**

These metrics help assess the accuracy and fluency of the generated captions.

---

## ğŸ“ˆ Future Enhancements
- Transformer-based captioning models
- Beam search decoding
- Pretrained vision encoders (ResNet, EfficientNet)
- CIDEr and SPICE metric integration
- Web-based demo for real-time caption generation

---

## ğŸ“š Learning Outcomes
- Encoderâ€“decoder architectures
- Attention mechanisms in deep learning
- CNN-based feature extraction
- Sequence modeling with RNNs / LSTMs
- NLP evaluation metrics
- Model interpretability using attention visualization

---

## ğŸ‘¤ Author
- Aryan Doshi
